{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d475e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1af3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset\n",
    "from src.trainer import Trainer\n",
    "from src.utils import set_seed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "# Package & visualize data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d0c11",
   "metadata": {},
   "source": [
    "# Experiment: \n",
    "\n",
    "It would be interseting to see if we keep the same performance as we scale the depth. In `1width.ipynb`, we were looking at a single layer NN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d60e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    1-Hidden Layer MLP for testing Kernel Regime\n",
    "\n",
    "    Assumptions:\n",
    "        - Input is MNIST images of size 28x28\n",
    "        - Output is 10 classes + auxiliary_logits\n",
    "    '''\n",
    "    def __init__(self, \n",
    "        hidden_width: int, \n",
    "        auxiliary_logits: int, \n",
    "        depth: int = 1,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        input_dim = 28*28\n",
    "        for i in range(depth):\n",
    "            layers.append(nn.Linear(input_dim, hidden_width))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_width\n",
    "        layers.append(nn.Linear(hidden_width, 10 + auxiliary_logits))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "def init_models(hidden_width: int, \n",
    "                auxiliary_logits: int, \n",
    "                depth: int):\n",
    "    teacher = MLP(hidden_width, auxiliary_logits, depth)\n",
    "    student = MLP(hidden_width, auxiliary_logits, depth)\n",
    "    student.load_state_dict(teacher.state_dict())\n",
    "    return teacher, student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(hidden_width: int, auxiliary_logits: int, seed: int = 42):\n",
    "  # Seed\n",
    "  g = set_seed(seed)\n",
    "\n",
    "  # Initalize Network\n",
    "  teacher, student = init_models(hidden_width=hidden_width, auxiliary_logits=auxiliary_logits)\n",
    "  # teacher = torch.compile(teacher)\n",
    "  # student = torch.compile(student)\n",
    "\n",
    "  # Training Parameters\n",
    "  epochs = 5\n",
    "  lr = 0.001\n",
    "  optimizer_teacher = torch.optim.Adam(teacher.parameters(), lr=lr)\n",
    "  optimizer_student = torch.optim.Adam(student.parameters(), lr=lr)\n",
    "  criterion_teacher = nn.CrossEntropyLoss()\n",
    "  criterion_student = nn.MSELoss()\n",
    "\n",
    "  batch_size = 128\n",
    "  train_loader, test_loader = Dataset.load_FashionMNIST(batch_size=batch_size, seed=seed)\n",
    "\n",
    "  # Run Training\n",
    "  trainer = Trainer(student, teacher, train_loader, optimizer_teacher, optimizer_student, criterion_teacher, criterion_student, device)\n",
    "\n",
    "  baseline_teacher = trainer.performance(trainer.teacher, test_loader)\n",
    "  baseline_student = trainer.performance(trainer.student, test_loader)\n",
    "\n",
    "  print('Start Training')\n",
    "  trainer.train_teacher(epochs)\n",
    "  trainer.train_student(epochs)\n",
    "  print('Finished Training')\n",
    "\n",
    "  results_teacher = trainer.performance(trainer.teacher, test_loader)\n",
    "  results_student = trainer.performance(trainer.student, test_loader)\n",
    "\n",
    "  del teacher, student, trainer, optimizer_student, optimizer_teacher\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  return baseline_teacher, baseline_student, results_teacher, results_student"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
